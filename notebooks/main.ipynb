{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baa6dae",
   "metadata": {},
   "source": [
    "# Email spam classification using Feedforward Neural Networks\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a5d45",
   "metadata": {},
   "source": [
    "In this project, our goal is to classify emails as either \"spam\" or \"not spam\" also known as ham. We will aim to use a feedforward neural network for Email spam classification is a common problem in natural language processing, where the objective is to automatically detect and filter out unwanted or potentially harmful messages.\n",
    "\n",
    "Usually I think we would use a more linear model such as a linear regression or Naive Bayes algorithm if we think the relationship between the features and the output is linear, or we think that the dataset is small and we worry about overfitting. But I would like to learn more about neural networks so I will try with a feedforward neural network approach, aiming to see if we can capture any complex interactions from features that may affect the output (for example image or speech recognition).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2f213",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8238cd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category    0\n",
      "Message     0\n",
      "dtype: int64\n",
      "Category\n",
      "ham               4825\n",
      "spam               747\n",
      "{\"mode\":\"full\"       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('email.csv')\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check unique labels\n",
    "print(df['Category'].value_counts())\n",
    "df = df.iloc[:-1]\n",
    "df = df[['Category', 'Message']]  # drop extra cols and keep the cols we want\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df1a35",
   "metadata": {},
   "source": [
    "The dataset has **5572 rows** and there's only two categories 'ham' and 'spam'. Now we want to label each email message and give it their true values under 'label', that way when we calculate the loss we know what the true value is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81e782d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  Label\n",
       "0      ham  Go until jurong point, crazy.. Available only ...      0\n",
       "1      ham                      Ok lar... Joking wif u oni...      0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
       "3      ham  U dun say so early hor... U c already then say...      0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...      0\n",
       "5     spam  FreeMsg Hey there darling it's been 3 week's n...      1\n",
       "6      ham  Even my brother is not like to speak with me. ...      0\n",
       "7      ham  As per your request 'Melle Melle (Oru Minnamin...      0\n",
       "8     spam  WINNER!! As a valued network customer you have...      1\n",
       "9     spam  Had your mobile 11 months or more? U R entitle...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5572, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Label'] = le.fit_transform(df['Category'])\n",
    "\n",
    "display(df.head(10))\n",
    "display(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554940c",
   "metadata": {},
   "source": [
    "## Split the data to training and testing data (80/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fed889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "1978    Reply to win Â£100 weekly! Where will the 2006 ...\n",
      "3989    Hello. Sort of out in town already. That . So ...\n",
      "3935     How come guoyang go n tell her? Then u told her?\n",
      "Name: Message, dtype: object (4457,)\n",
      "1978    1\n",
      "3989    0\n",
      "3935    0\n",
      "Name: Label, dtype: int64 (4457,)\n",
      "Testing set:\n",
      "3245    Squeeeeeze!! This is christmas hug.. If u lik ...\n",
      "944     And also I've sorta blown him off a couple tim...\n",
      "1044    Mmm thats better now i got a roast down me! iÂ...\n",
      "Name: Message, dtype: object (1115,)\n",
      "3245    0\n",
      "944     0\n",
      "1044    0\n",
      "Name: Label, dtype: int64 (1115,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Message'],     # input: raw messages\n",
    "    df['Label'],       # target: 0 = ham, 1 = spam\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(X_train[:3], X_train.shape)\n",
    "print(y_train[:3], y_train.shape)\n",
    "\n",
    "print(\"Testing set:\")\n",
    "print(X_test[:3], X_test.shape)\n",
    "print(y_test[:3], y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfde1e8",
   "metadata": {},
   "source": [
    "Here, we see that X is the messages (which will be converted to vectorised forms), and y is the label value (1s and 0s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7f59c",
   "metadata": {},
   "source": [
    "## Vectorisation (Converting word to vectors/numerical values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d881f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 3000) (4457,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_train_vec.shape, X_test_vec.shape\n",
    "\n",
    "\n",
    "#convert to dense matrix because pytorch/tensorflow doesn't support sparse matrices\n",
    "X_train_dense = X_train_vec.toarray()\n",
    "X_test_dense = X_test_vec.toarray()\n",
    "\n",
    "\n",
    "X_train_dense.shape,y_train.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "sample 1: [0.0042, 0.0042, 0.0042, ...] (3000 features/columns) = 1 label (0 or 1)\n",
    "sample 2: [0.0042, 0.0042, 0.0042, ...] (3000 features/columns) = 1 label (0 or 1)\n",
    "sample 3: [0.0042, 0.0042, 0.0042, ...] (3000 features/columns) = 1 label (0 or 1)\n",
    "...\n",
    "...\n",
    "...\n",
    "sample 4457: [0.0042, 0.0042, 0.0042, ...] (3000 features/columns) = 1 label (0 or 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print(X_train_dense.shape,y_train.shape)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f2aed",
   "metadata": {},
   "source": [
    "# Convert to tensors and batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db226e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([1.]))\n",
      "70\n",
      "18\n",
      "torch.Size([64, 3000])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "#convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_dense, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)  # shape (N, 1)\n",
    "X_test_tensor = torch.tensor(X_test_dense, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)    # shape (N, 1)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#combine tensors into a dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "print(train_dataset[0])\n",
    "\n",
    "#batch them up\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "#we can see we've batched it into 70 training batches and 18 testing batches\n",
    "\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)  # should now be torch.Size([64, 1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc2e31",
   "metadata": {},
   "source": [
    "Input size: 3000 (Each batch has 64 samples and 3000 features)\n",
    "First layer: 128 hidden neurons/units/features\n",
    "Second layer: 64 hidden neurons\n",
    "\n",
    "Output layer: 1 neuron (since we want to do binary classification). But if we were doing something like letter prediction, we might do 26 neuron output (A-Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7cad9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf530cf5",
   "metadata": {},
   "source": [
    "# Model Architecture FFNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SpamClassifier(nn.Module):\n",
    "\n",
    "\n",
    "    #input = n_features (3000)\n",
    "    #hidden layers = n_hidden\n",
    "    #output = n_output\n",
    "    def __init__(self,n_features,n_hidden=[128,64],n_output=1):\n",
    "        #initialize the model\n",
    "        super(SpamClassifier,self).__init__()\n",
    "\n",
    "        in_features = n_features\n",
    "        layers = []\n",
    "\n",
    "        #for each hidden layer, we add a linear layer and a ReLU activation function. \n",
    "        #Input layer will have 3000 features\n",
    "        #1st hidden layer will have 128 features\n",
    "        #2nd hidden layer will have 64 features\n",
    "\n",
    "        for hidden_layer in n_hidden:\n",
    "            layers.append(nn.Linear(in_features,hidden_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = hidden_layer\n",
    "\n",
    "        #output layer will have 1 feature\n",
    "        layers.append(nn.Linear(in_features,n_output))\n",
    "        #use sigmoid rather than softmax cos binary classification\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "\n",
    "        #create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa970853",
   "metadata": {},
   "source": [
    "# Set up model, optimiser, and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7849b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#the number of features \n",
    "input_size = X_batch.shape[1]\n",
    "hidden_layers = [128,64]\n",
    "output_size = 1\n",
    "\n",
    "#create the model\n",
    "ffnn_model = SpamClassifier(input_size,hidden_layers,output_size).to(device)\n",
    "\n",
    "#loss function\n",
    "loss_fn = nn.BCELoss() \n",
    "\n",
    "#optimiser\n",
    "learning_rate = 0.001\n",
    "optimiser = optim.AdamW(ffnn_model.parameters(),lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f42962",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2860e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.4450, Accuracy: 86.58%\n",
      "Epoch 2/50, Loss: 0.1240, Accuracy: 91.09%\n",
      "Epoch 3/50, Loss: 0.0373, Accuracy: 93.73%\n",
      "Epoch 4/50, Loss: 0.0162, Accuracy: 95.19%\n",
      "Epoch 5/50, Loss: 0.0088, Accuracy: 96.11%\n",
      "Epoch 6/50, Loss: 0.0055, Accuracy: 96.75%\n",
      "Epoch 7/50, Loss: 0.0044, Accuracy: 97.20%\n",
      "Epoch 8/50, Loss: 0.0037, Accuracy: 97.54%\n",
      "Epoch 9/50, Loss: 0.0035, Accuracy: 97.81%\n",
      "Epoch 10/50, Loss: 0.0027, Accuracy: 98.02%\n",
      "Epoch 11/50, Loss: 0.0024, Accuracy: 98.20%\n",
      "Epoch 12/50, Loss: 0.0021, Accuracy: 98.35%\n",
      "Epoch 13/50, Loss: 0.0019, Accuracy: 98.47%\n",
      "Epoch 14/50, Loss: 0.0019, Accuracy: 98.58%\n",
      "Epoch 15/50, Loss: 0.0017, Accuracy: 98.67%\n",
      "Epoch 16/50, Loss: 0.0017, Accuracy: 98.75%\n",
      "Epoch 17/50, Loss: 0.0016, Accuracy: 98.83%\n",
      "Epoch 18/50, Loss: 0.0016, Accuracy: 98.89%\n",
      "Epoch 19/50, Loss: 0.0016, Accuracy: 98.95%\n",
      "Epoch 20/50, Loss: 0.0015, Accuracy: 99.00%\n",
      "Epoch 21/50, Loss: 0.0015, Accuracy: 99.04%\n",
      "Epoch 22/50, Loss: 0.0015, Accuracy: 99.09%\n",
      "Epoch 23/50, Loss: 0.0015, Accuracy: 99.13%\n",
      "Epoch 24/50, Loss: 0.0014, Accuracy: 99.16%\n",
      "Epoch 25/50, Loss: 0.0014, Accuracy: 99.19%\n",
      "Epoch 26/50, Loss: 0.0014, Accuracy: 99.22%\n",
      "Epoch 27/50, Loss: 0.0014, Accuracy: 99.25%\n",
      "Epoch 28/50, Loss: 0.0014, Accuracy: 99.28%\n",
      "Epoch 29/50, Loss: 0.0014, Accuracy: 99.30%\n",
      "Epoch 30/50, Loss: 0.0014, Accuracy: 99.32%\n",
      "Epoch 31/50, Loss: 0.0014, Accuracy: 99.35%\n",
      "Epoch 32/50, Loss: 0.0014, Accuracy: 99.37%\n",
      "Epoch 33/50, Loss: 0.0014, Accuracy: 99.38%\n",
      "Epoch 34/50, Loss: 0.0013, Accuracy: 99.40%\n",
      "Epoch 35/50, Loss: 0.0013, Accuracy: 99.42%\n",
      "Epoch 36/50, Loss: 0.0014, Accuracy: 99.43%\n",
      "Epoch 37/50, Loss: 0.0013, Accuracy: 99.45%\n",
      "Epoch 38/50, Loss: 0.0013, Accuracy: 99.46%\n",
      "Epoch 39/50, Loss: 0.0013, Accuracy: 99.48%\n",
      "Epoch 40/50, Loss: 0.0013, Accuracy: 99.49%\n",
      "Epoch 41/50, Loss: 0.0014, Accuracy: 99.50%\n",
      "Epoch 42/50, Loss: 0.0013, Accuracy: 99.51%\n",
      "Epoch 43/50, Loss: 0.0013, Accuracy: 99.52%\n",
      "Epoch 44/50, Loss: 0.0013, Accuracy: 99.53%\n",
      "Epoch 45/50, Loss: 0.0013, Accuracy: 99.54%\n",
      "Epoch 46/50, Loss: 0.0013, Accuracy: 99.55%\n",
      "Epoch 47/50, Loss: 0.0013, Accuracy: 99.56%\n",
      "Epoch 48/50, Loss: 0.0013, Accuracy: 99.57%\n",
      "Epoch 49/50, Loss: 0.0013, Accuracy: 99.58%\n",
      "Epoch 50/50, Loss: 0.0013, Accuracy: 99.59%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "correct = 0\n",
    "total = 0\n",
    "for epoch in range(num_epochs):\n",
    "    ffnn_model.train() #set the model to training mode\n",
    "    running_loss=0\n",
    "\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #using CUDA\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        #forward pass\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        loss = loss_fn(outputs,y_batch)\n",
    "\n",
    "\n",
    "        #calculate accuracy\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        #backward pass\n",
    "        #the goal here is to update the weights of the model by calculating the gradient of the loss function with respect to the weights\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c212a",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b38a7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.83%\n"
     ]
    }
   ],
   "source": [
    "ffnn_model.eval()  # set to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "        test_total += y_batch.size(0)\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe71c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.99      0.99      0.99       966\n",
      "        Spam       0.96      0.95      0.96       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.97      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Collect all predictions and labels\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        y_preds.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names=['Ham', 'Spam']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d9ec5",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "torch.save(ffnn_model.state_dict(), 'ffnn_model.pth')\n",
    "with open('tfidf_vectoriser.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
