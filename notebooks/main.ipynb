{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baa6dae",
   "metadata": {},
   "source": [
    "# Email spam classification using Feedforward Neural Networks\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a5d45",
   "metadata": {},
   "source": [
    "In this project, our goal is to classify emails as either \"spam\" or \"not spam\" also known as ham. We will aim to use a feedforward neural network for Email spam classification is a common problem in natural language processing, where the objective is to automatically detect and filter out unwanted or potentially harmful messages.\n",
    "\n",
    "Usually I think we would use a more linear model such as a linear regression or Naive Bayes algorithm if we think the relationship between the features and the output is linear, or we think that the dataset is small and we worry about overfitting. But I would like to learn more about neural networks so I will try with a feedforward neural network approach, aiming to see if we can capture any complex interactions from features that may affect the output (for example image or speech recognition).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2f213",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8238cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "df1 = load_data('../datasets/email.csv')\n",
    "df2 = load_data('../datasets/email_spam2.csv')\n",
    "\n",
    "\n",
    "df1 = df1.iloc[:-1]\n",
    "df1 = df1[['Category', 'Message']]  # drop extra cols and keep the cols we want\n",
    "df2 = df2[['Category', 'Message']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b81e782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    4883\n",
      "1     773\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df1['Label'] = le.fit_transform(df1['Category'])\n",
    "\n",
    "df2['Label'] = le.fit_transform(df2['Category'])\n",
    "df =pd.concat([df1, df2], ignore_index=True)\n",
    "print(df['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554940c",
   "metadata": {},
   "source": [
    "## Split the data to training, validation, and testing data (70/10/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1fed889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "(3958,)\n",
      "(3958,)\n",
      "Validation set:\n",
      "(566,)\n",
      "(566,)\n",
      "Testing set:\n",
      "(1132,)\n",
      "(1132,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Split into training, validation and testing sets (70/10/20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df['Message'],     # input: raw messages\n",
    "    df['Label'],       # target: 0 = ham, 1 = spam\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.125,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set:\")\n",
    "print( X_train.shape)\n",
    "print( y_train.shape)\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print( X_val.shape)\n",
    "print( y_val.shape)\n",
    "\n",
    "print(\"Testing set:\")\n",
    "print(X_test.shape)\n",
    "print( y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7f59c",
   "metadata": {},
   "source": [
    "## Vectorisation (Converting word to vectors/numerical values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76d881f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3958, 7915), (3958,)\n",
      "Testing set: (1132, 7915), (1132,)\n",
      "Validation set: (566, 7915), (566,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=8000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_train_vec.shape, X_test_vec.shape, X_val_vec.shape\n",
    "\n",
    "\n",
    "#convert to dense matrix because pytorch/tensorflow doesn't support sparse matrices\n",
    "X_train_dense = X_train_vec.toarray()\n",
    "X_test_dense = X_test_vec.toarray()\n",
    "X_val_dense = X_val_vec.toarray()\n",
    "X_train_dense.shape,y_train.shape, X_test_dense.shape, X_val_dense.shape\n",
    "\n",
    "print(f\"Training set: {X_train_dense.shape}, {y_train.shape}\")    \n",
    "print(f\"Testing set: {X_test_dense.shape}, {y_test.shape}\")    \n",
    "print(f\"Validation set: {X_val_dense.shape}, {y_val.shape}\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f2aed",
   "metadata": {},
   "source": [
    "# Convert to tensors and batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db226e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([0.]))\n",
      "62\n",
      "18\n",
      "9\n",
      "torch.Size([64, 7915])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "#convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_dense, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)  # shape (N, 1)\n",
    "X_test_tensor = torch.tensor(X_test_dense, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)    # shape (N, 1)\n",
    "X_val_tensor = torch.tensor(X_val_dense, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)    # shape (N, 1)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#combine tensors into a dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "print(train_dataset[0])\n",
    "\n",
    "#batch them up\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "print(len(val_loader))\n",
    "#we can see we've batched it into 70 training batches and 18 testing batches\n",
    "\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)  # should now be torch.Size([64, 1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc2e31",
   "metadata": {},
   "source": [
    "Input size: 8000 (Each batch has 64 samples and 8000 features)\n",
    "First layer: 256 hidden neurons/units/features\n",
    "Second layer: 128 hidden neurons\n",
    "\n",
    "Output layer: 1 neuron (since we want to do binary classification). But if we were doing something like letter prediction, we might do 26 neuron output (A-Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7cad9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf530cf5",
   "metadata": {},
   "source": [
    "# Model Architecture FFNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "674a2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SpamClassifier(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,n_features,n_hidden=[256,128],n_output=1):\n",
    "        #initialize the model\n",
    "        super(SpamClassifier,self).__init__()\n",
    "\n",
    "        in_features = n_features\n",
    "        layers = []\n",
    "        #for each hidden layer, we add a linear layer and a ReLU activation function. \n",
    "        #Input layer will have 8000 features\n",
    "        #1st hidden layer will have 256 features\n",
    "        #2nd hidden layer will have 128 features\n",
    "\n",
    "        for hidden_layer in n_hidden:\n",
    "            layers.append(nn.Linear(in_features,hidden_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            in_features = hidden_layer\n",
    "\n",
    "        #output layer will have 1 feature\n",
    "        layers.append(nn.Linear(in_features,n_output))\n",
    "        #use sigmoid rather than softmax cos binary classification\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "\n",
    "        #create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa970853",
   "metadata": {},
   "source": [
    "# Set up model, optimiser, and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7849b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]       2,026,496\n",
      "              ReLU-2                  [-1, 256]               0\n",
      "            Linear-3                  [-1, 128]          32,896\n",
      "              ReLU-4                  [-1, 128]               0\n",
      "            Linear-5                    [-1, 1]             129\n",
      "           Sigmoid-6                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,059,521\n",
      "Trainable params: 2,059,521\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 7.86\n",
      "Estimated Total Size (MB): 7.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#the number of features \n",
    "input_size = X_batch.shape[1]\n",
    "hidden_layers = [256,128]\n",
    "output_size = 1\n",
    "\n",
    "#create the model\n",
    "ffnn_model = SpamClassifier(input_size,hidden_layers,output_size).to(device)\n",
    "import torchsummary\n",
    "torchsummary.summary(ffnn_model, (input_size,))\n",
    "\n",
    "#loss function\n",
    "loss_fn = nn.BCELoss() \n",
    "\n",
    "#optimiser\n",
    "learning_rate = 0.0001\n",
    "optimiser = optim.AdamW(ffnn_model.parameters(),lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f42962",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2860e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.6658, Train Acc: 86.18% | Val Loss: 0.6466, Val Acc: 87.81%\n",
      "Epoch 2/50 | Train Loss: 0.6189, Train Acc: 86.18% | Val Loss: 0.5730, Val Acc: 87.81%\n",
      "Epoch 3/50 | Train Loss: 0.5115, Train Acc: 86.18% | Val Loss: 0.4275, Val Acc: 87.81%\n",
      "Epoch 4/50 | Train Loss: 0.3574, Train Acc: 86.31% | Val Loss: 0.2859, Val Acc: 88.69%\n",
      "Epoch 5/50 | Train Loss: 0.2409, Train Acc: 90.96% | Val Loss: 0.2071, Val Acc: 93.29%\n",
      "Epoch 6/50 | Train Loss: 0.1754, Train Acc: 94.64% | Val Loss: 0.1652, Val Acc: 95.41%\n",
      "Epoch 7/50 | Train Loss: 0.1320, Train Acc: 96.61% | Val Loss: 0.1368, Val Acc: 96.47%\n",
      "Epoch 8/50 | Train Loss: 0.0981, Train Acc: 97.95% | Val Loss: 0.1151, Val Acc: 96.47%\n",
      "Epoch 9/50 | Train Loss: 0.0722, Train Acc: 98.74% | Val Loss: 0.1014, Val Acc: 97.00%\n",
      "Epoch 10/50 | Train Loss: 0.0539, Train Acc: 99.19% | Val Loss: 0.0921, Val Acc: 97.17%\n",
      "Epoch 11/50 | Train Loss: 0.0410, Train Acc: 99.37% | Val Loss: 0.0878, Val Acc: 97.17%\n",
      "Epoch 12/50 | Train Loss: 0.0322, Train Acc: 99.57% | Val Loss: 0.0843, Val Acc: 97.17%\n",
      "Epoch 13/50 | Train Loss: 0.0259, Train Acc: 99.60% | Val Loss: 0.0841, Val Acc: 97.17%\n",
      "Epoch 14/50 | Train Loss: 0.0210, Train Acc: 99.67% | Val Loss: 0.0830, Val Acc: 97.17%\n",
      "Epoch 15/50 | Train Loss: 0.0175, Train Acc: 99.70% | Val Loss: 0.0826, Val Acc: 97.17%\n",
      "Epoch 16/50 | Train Loss: 0.0147, Train Acc: 99.77% | Val Loss: 0.0822, Val Acc: 97.17%\n",
      "Epoch 17/50 | Train Loss: 0.0126, Train Acc: 99.80% | Val Loss: 0.0827, Val Acc: 97.17%\n",
      "Epoch 18/50 | Train Loss: 0.0107, Train Acc: 99.85% | Val Loss: 0.0839, Val Acc: 97.17%\n",
      "Epoch 19/50 | Train Loss: 0.0094, Train Acc: 99.82% | Val Loss: 0.0842, Val Acc: 97.35%\n",
      "Epoch 20/50 | Train Loss: 0.0081, Train Acc: 99.90% | Val Loss: 0.0869, Val Acc: 97.35%\n",
      "Epoch 21/50 | Train Loss: 0.0071, Train Acc: 99.92% | Val Loss: 0.0884, Val Acc: 97.35%\n",
      "Epoch 22/50 | Train Loss: 0.0063, Train Acc: 99.95% | Val Loss: 0.0875, Val Acc: 97.35%\n",
      "Epoch 23/50 | Train Loss: 0.0056, Train Acc: 99.95% | Val Loss: 0.0894, Val Acc: 97.35%\n",
      "Epoch 24/50 | Train Loss: 0.0050, Train Acc: 99.92% | Val Loss: 0.0919, Val Acc: 97.35%\n",
      "Epoch 25/50 | Train Loss: 0.0044, Train Acc: 99.95% | Val Loss: 0.0922, Val Acc: 97.35%\n",
      "Epoch 26/50 | Train Loss: 0.0039, Train Acc: 99.97% | Val Loss: 0.0919, Val Acc: 97.35%\n",
      "Epoch 27/50 | Train Loss: 0.0035, Train Acc: 99.97% | Val Loss: 0.0924, Val Acc: 97.35%\n",
      "Epoch 28/50 | Train Loss: 0.0032, Train Acc: 99.97% | Val Loss: 0.0948, Val Acc: 97.35%\n",
      "Epoch 29/50 | Train Loss: 0.0029, Train Acc: 99.97% | Val Loss: 0.0948, Val Acc: 97.35%\n",
      "Epoch 30/50 | Train Loss: 0.0026, Train Acc: 99.97% | Val Loss: 0.0956, Val Acc: 97.35%\n",
      "Epoch 31/50 | Train Loss: 0.0024, Train Acc: 99.97% | Val Loss: 0.0970, Val Acc: 97.35%\n",
      "Epoch 32/50 | Train Loss: 0.0022, Train Acc: 100.00% | Val Loss: 0.0994, Val Acc: 97.35%\n",
      "Epoch 33/50 | Train Loss: 0.0020, Train Acc: 100.00% | Val Loss: 0.0993, Val Acc: 97.35%\n",
      "Epoch 34/50 | Train Loss: 0.0018, Train Acc: 100.00% | Val Loss: 0.1070, Val Acc: 97.35%\n",
      "Epoch 35/50 | Train Loss: 0.0017, Train Acc: 100.00% | Val Loss: 0.1024, Val Acc: 97.35%\n",
      "Epoch 36/50 | Train Loss: 0.0015, Train Acc: 100.00% | Val Loss: 0.1024, Val Acc: 97.35%\n",
      "Epoch 37/50 | Train Loss: 0.0014, Train Acc: 100.00% | Val Loss: 0.1036, Val Acc: 97.35%\n",
      "Epoch 38/50 | Train Loss: 0.0013, Train Acc: 100.00% | Val Loss: 0.1079, Val Acc: 97.35%\n",
      "Epoch 39/50 | Train Loss: 0.0012, Train Acc: 100.00% | Val Loss: 0.1083, Val Acc: 97.35%\n",
      "Epoch 40/50 | Train Loss: 0.0012, Train Acc: 100.00% | Val Loss: 0.1068, Val Acc: 97.35%\n",
      "Epoch 41/50 | Train Loss: 0.0011, Train Acc: 100.00% | Val Loss: 0.1086, Val Acc: 97.35%\n",
      "Epoch 42/50 | Train Loss: 0.0010, Train Acc: 100.00% | Val Loss: 0.1106, Val Acc: 97.35%\n",
      "Epoch 43/50 | Train Loss: 0.0010, Train Acc: 100.00% | Val Loss: 0.1111, Val Acc: 97.35%\n",
      "Epoch 44/50 | Train Loss: 0.0009, Train Acc: 100.00% | Val Loss: 0.1141, Val Acc: 97.35%\n",
      "Epoch 45/50 | Train Loss: 0.0008, Train Acc: 100.00% | Val Loss: 0.1112, Val Acc: 97.35%\n",
      "Epoch 46/50 | Train Loss: 0.0008, Train Acc: 100.00% | Val Loss: 0.1168, Val Acc: 97.35%\n",
      "Epoch 47/50 | Train Loss: 0.0007, Train Acc: 100.00% | Val Loss: 0.1158, Val Acc: 97.35%\n",
      "Epoch 48/50 | Train Loss: 0.0007, Train Acc: 100.00% | Val Loss: 0.1148, Val Acc: 97.35%\n",
      "Epoch 49/50 | Train Loss: 0.0007, Train Acc: 100.00% | Val Loss: 0.1175, Val Acc: 97.35%\n",
      "Epoch 50/50 | Train Loss: 0.0006, Train Acc: 100.00% | Val Loss: 0.1189, Val Acc: 97.35%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "for epoch in range(num_epochs):\n",
    "    ffnn_model.train() #set the model to training mode\n",
    "    running_loss=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        #forward pass\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        loss = loss_fn(outputs,y_batch)\n",
    "\n",
    "        #backward pass\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "            #calculate accuracy\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "       \n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    train_loss = running_loss/len(train_loader)\n",
    "\n",
    "    ffnn_model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch = X_val_batch.to(device)\n",
    "            y_val_batch = y_val_batch.to(device)\n",
    "\n",
    "            val_outputs = ffnn_model(X_val_batch)\n",
    "            val_loss += loss_fn(val_outputs, y_val_batch).item()\n",
    "\n",
    "            val_preds = (val_outputs > 0.5).float()\n",
    "            val_correct += (val_preds == y_val_batch).sum().item()\n",
    "            val_total += y_val_batch.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss_avg = val_loss / len(val_loader)\n",
    "\n",
    "    # Save model if this epoch has the best validation accuracy so far\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        best_model_state = ffnn_model.state_dict()  #\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy*100:.2f}% | Val Loss: {val_loss_avg:.4f}, Val Acc: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c212a",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6dbe71c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.98      1.00      0.99       975\n",
      "        Spam       0.98      0.90      0.94       157\n",
      "\n",
      "    accuracy                           0.98      1132\n",
      "   macro avg       0.98      0.95      0.97      1132\n",
      "weighted avg       0.98      0.98      0.98      1132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Collect all predictions and labels\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        predicted = (outputs > 0.5).int()\n",
    "\n",
    "        y_preds.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names=['Ham', 'Spam']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d9ec5",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c668e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure the weights directory exists\n",
    "os.makedirs('../weights', exist_ok=True)\n",
    "\n",
    "# Save the model weights to ../weights\n",
    "torch.save(ffnn_model.state_dict(), '../weights/ffnn_model_v3.pth')\n",
    "with open('../vectorisers/tfidf_vectoriser_v3.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
