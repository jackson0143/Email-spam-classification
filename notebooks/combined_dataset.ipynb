{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baa6dae",
   "metadata": {},
   "source": [
    "# Email spam classification using Feedforward Neural Networks\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a5d45",
   "metadata": {},
   "source": [
    "In this project, our goal is to classify emails as either \"spam\" or \"not spam\" also known as ham. We will aim to use a feedforward neural network for Email spam classification is a common problem in natural language processing, where the objective is to automatically detect and filter out unwanted or potentially harmful messages.\n",
    "\n",
    "Usually I think we would use a more linear model such as a linear regression or Naive Bayes algorithm if we think the relationship between the features and the output is linear, or we think that the dataset is small and we worry about overfitting. But I would like to learn more about neural networks so I will try with a feedforward neural network approach, aiming to see if we can capture any complex interactions from features that may affect the output (for example image or speech recognition).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2f213",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8238cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "df1 = load_data('../datasets/email.csv')\n",
    "df2 = load_data('../datasets/email_spam2.csv')\n",
    "\n",
    "\n",
    "df1 = df1.iloc[:-1]\n",
    "df1 = df1[['Category', 'Message']]  # drop extra cols and keep the cols we want\n",
    "df2 = df2[['Category', 'Message']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81e782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    4883\n",
      "1     773\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df1['Label'] = le.fit_transform(df1['Category'])\n",
    "\n",
    "df2['Label'] = le.fit_transform(df2['Category'])\n",
    "df =pd.concat([df1, df2], ignore_index=True)\n",
    "print(df['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554940c",
   "metadata": {},
   "source": [
    "## Split the data to training and testing data (80/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fed889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "5028    PRIVATE! Your 2003 Account Statement for shows...\n",
      "2767    Married local women looking for discreet actio...\n",
      "1620              Friends that u can stay on fb chat with\n",
      "Name: Message, dtype: object (4524,)\n",
      "5028    1\n",
      "2767    1\n",
      "1620    0\n",
      "Name: Label, dtype: int64 (4524,)\n",
      "Testing set:\n",
      "3948                 Sorry, went to bed early, nightnight\n",
      "5097    Sorry about that this is my mates phone and i ...\n",
      "3689                           I'll meet you in the lobby\n",
      "Name: Message, dtype: object (1132,)\n",
      "3948    0\n",
      "5097    0\n",
      "3689    0\n",
      "Name: Label, dtype: int64 (1132,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Message'],     # input: raw messages\n",
    "    df['Label'],       # target: 0 = ham, 1 = spam\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(X_train[:3], X_train.shape)\n",
    "print(y_train[:3], y_train.shape)\n",
    "\n",
    "print(\"Testing set:\")\n",
    "print(X_test[:3], X_test.shape)\n",
    "print(y_test[:3], y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfde1e8",
   "metadata": {},
   "source": [
    "Here, we see that X is the messages (which will be converted to vectorised forms), and y is the label value (1s and 0s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7f59c",
   "metadata": {},
   "source": [
    "## Vectorisation (Converting word to vectors/numerical values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d881f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4524, 3000) (4524,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_train_vec.shape, X_test_vec.shape\n",
    "\n",
    "\n",
    "#convert to dense matrix because pytorch/tensorflow doesn't support sparse matrices\n",
    "X_train_dense = X_train_vec.toarray()\n",
    "X_test_dense = X_test_vec.toarray()\n",
    "X_train_dense.shape,y_train.shape\n",
    "\n",
    "print(X_train_dense.shape,y_train.shape)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f2aed",
   "metadata": {},
   "source": [
    "# Convert to tensors and batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db226e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([1.]))\n",
      "71\n",
      "18\n",
      "torch.Size([64, 3000])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "#convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_dense, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)  # shape (N, 1)\n",
    "X_test_tensor = torch.tensor(X_test_dense, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)    # shape (N, 1)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#combine tensors into a dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "print(train_dataset[0])\n",
    "\n",
    "#batch them up\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "#we can see we've batched it into 70 training batches and 18 testing batches\n",
    "\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)  # should now be torch.Size([64, 1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc2e31",
   "metadata": {},
   "source": [
    "Input size: 3000 (Each batch has 64 samples and 3000 features)\n",
    "First layer: 128 hidden neurons/units/features\n",
    "Second layer: 64 hidden neurons\n",
    "\n",
    "Output layer: 1 neuron (since we want to do binary classification). But if we were doing something like letter prediction, we might do 26 neuron output (A-Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7cad9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf530cf5",
   "metadata": {},
   "source": [
    "# Model Architecture FFNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674a2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SpamClassifier(nn.Module):\n",
    "\n",
    "\n",
    "    #input = n_features (3000)\n",
    "    #hidden layers = n_hidden\n",
    "    #output = n_output\n",
    "    def __init__(self,n_features,n_hidden=[128,64],n_output=1):\n",
    "        #initialize the model\n",
    "        super(SpamClassifier,self).__init__()\n",
    "\n",
    "        in_features = n_features\n",
    "        layers = []\n",
    "\n",
    "        #for each hidden layer, we add a linear layer and a ReLU activation function. \n",
    "        #Input layer will have 3000 features\n",
    "        #1st hidden layer will have 128 features\n",
    "        #2nd hidden layer will have 64 features\n",
    "\n",
    "        for hidden_layer in n_hidden:\n",
    "            layers.append(nn.Linear(in_features,hidden_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = hidden_layer\n",
    "\n",
    "        #output layer will have 1 feature\n",
    "        layers.append(nn.Linear(in_features,n_output))\n",
    "        #use sigmoid rather than softmax cos binary classification\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "\n",
    "        #create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa970853",
   "metadata": {},
   "source": [
    "# Set up model, optimiser, and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7849b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]         384,128\n",
      "              ReLU-2                  [-1, 128]               0\n",
      "            Linear-3                   [-1, 64]           8,256\n",
      "              ReLU-4                   [-1, 64]               0\n",
      "            Linear-5                    [-1, 1]              65\n",
      "           Sigmoid-6                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 392,449\n",
      "Trainable params: 392,449\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 1.50\n",
      "Estimated Total Size (MB): 1.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#the number of features \n",
    "input_size = X_batch.shape[1]\n",
    "hidden_layers = [128,64]\n",
    "output_size = 1\n",
    "\n",
    "#create the model\n",
    "ffnn_model = SpamClassifier(input_size,hidden_layers,output_size).to(device)\n",
    "import torchsummary\n",
    "torchsummary.summary(ffnn_model, (input_size,))\n",
    "\n",
    "#loss function\n",
    "loss_fn = nn.BCELoss() \n",
    "\n",
    "#optimiser\n",
    "learning_rate = 0.001\n",
    "optimiser = optim.AdamW(ffnn_model.parameters(),lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f42962",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2860e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.4808, Accuracy: 86.38%\n",
      "Epoch 2/50, Loss: 0.1475, Accuracy: 90.42%\n",
      "Epoch 3/50, Loss: 0.0535, Accuracy: 93.22%\n",
      "Epoch 4/50, Loss: 0.0229, Accuracy: 94.76%\n",
      "Epoch 5/50, Loss: 0.0137, Accuracy: 95.73%\n",
      "Epoch 6/50, Loss: 0.0099, Accuracy: 96.41%\n",
      "Epoch 7/50, Loss: 0.0083, Accuracy: 96.90%\n",
      "Epoch 8/50, Loss: 0.0074, Accuracy: 97.26%\n",
      "Epoch 9/50, Loss: 0.0067, Accuracy: 97.54%\n",
      "Epoch 10/50, Loss: 0.0063, Accuracy: 97.77%\n",
      "Epoch 11/50, Loss: 0.0058, Accuracy: 97.96%\n",
      "Epoch 12/50, Loss: 0.0057, Accuracy: 98.12%\n",
      "Epoch 13/50, Loss: 0.0055, Accuracy: 98.25%\n",
      "Epoch 14/50, Loss: 0.0053, Accuracy: 98.36%\n",
      "Epoch 15/50, Loss: 0.0048, Accuracy: 98.46%\n",
      "Epoch 16/50, Loss: 0.0046, Accuracy: 98.55%\n",
      "Epoch 17/50, Loss: 0.0046, Accuracy: 98.63%\n",
      "Epoch 18/50, Loss: 0.0048, Accuracy: 98.70%\n",
      "Epoch 19/50, Loss: 0.0043, Accuracy: 98.76%\n",
      "Epoch 20/50, Loss: 0.0042, Accuracy: 98.81%\n",
      "Epoch 21/50, Loss: 0.0042, Accuracy: 98.87%\n",
      "Epoch 22/50, Loss: 0.0040, Accuracy: 98.91%\n",
      "Epoch 23/50, Loss: 0.0039, Accuracy: 98.95%\n",
      "Epoch 24/50, Loss: 0.0040, Accuracy: 98.99%\n",
      "Epoch 25/50, Loss: 0.0040, Accuracy: 99.03%\n",
      "Epoch 26/50, Loss: 0.0043, Accuracy: 99.06%\n",
      "Epoch 27/50, Loss: 0.0038, Accuracy: 99.09%\n",
      "Epoch 28/50, Loss: 0.0039, Accuracy: 99.12%\n",
      "Epoch 29/50, Loss: 0.0039, Accuracy: 99.14%\n",
      "Epoch 30/50, Loss: 0.0039, Accuracy: 99.17%\n",
      "Epoch 31/50, Loss: 0.0036, Accuracy: 99.19%\n",
      "Epoch 32/50, Loss: 0.0037, Accuracy: 99.22%\n",
      "Epoch 33/50, Loss: 0.0038, Accuracy: 99.23%\n",
      "Epoch 34/50, Loss: 0.0037, Accuracy: 99.25%\n",
      "Epoch 35/50, Loss: 0.0039, Accuracy: 99.27%\n",
      "Epoch 36/50, Loss: 0.0040, Accuracy: 99.29%\n",
      "Epoch 37/50, Loss: 0.0039, Accuracy: 99.31%\n",
      "Epoch 38/50, Loss: 0.0043, Accuracy: 99.32%\n",
      "Epoch 39/50, Loss: 0.0038, Accuracy: 99.33%\n",
      "Epoch 40/50, Loss: 0.0036, Accuracy: 99.35%\n",
      "Epoch 41/50, Loss: 0.0035, Accuracy: 99.36%\n",
      "Epoch 42/50, Loss: 0.0034, Accuracy: 99.37%\n",
      "Epoch 43/50, Loss: 0.0037, Accuracy: 99.39%\n",
      "Epoch 44/50, Loss: 0.0037, Accuracy: 99.40%\n",
      "Epoch 45/50, Loss: 0.0036, Accuracy: 99.41%\n",
      "Epoch 46/50, Loss: 0.0037, Accuracy: 99.42%\n",
      "Epoch 47/50, Loss: 0.0040, Accuracy: 99.43%\n",
      "Epoch 48/50, Loss: 0.0048, Accuracy: 99.44%\n",
      "Epoch 49/50, Loss: 0.0036, Accuracy: 99.45%\n",
      "Epoch 50/50, Loss: 0.0035, Accuracy: 99.46%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "correct = 0\n",
    "total = 0\n",
    "for epoch in range(num_epochs):\n",
    "    ffnn_model.train() #set the model to training mode\n",
    "    running_loss=0\n",
    "\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #using CUDA\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        #forward pass\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        loss = loss_fn(outputs,y_batch)\n",
    "\n",
    "\n",
    "        #calculate accuracy\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        #backward pass\n",
    "        #the goal here is to update the weights of the model by calculating the gradient of the loss function with respect to the weights\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c212a",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "evaluate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38a7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.88%\n"
     ]
    }
   ],
   "source": [
    "ffnn_model.eval()  # set to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "        test_total += y_batch.size(0)\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dbe71c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.98      0.99      0.99       975\n",
      "        Spam       0.95      0.90      0.92       157\n",
      "\n",
      "    accuracy                           0.98      1132\n",
      "   macro avg       0.97      0.94      0.95      1132\n",
      "weighted avg       0.98      0.98      0.98      1132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Collect all predictions and labels\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = ffnn_model(X_batch)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        y_preds.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names=['Ham', 'Spam']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d9ec5",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure the weights directory exists\n",
    "os.makedirs('../weights', exist_ok=True)\n",
    "\n",
    "# Save the model weights to ../weights\n",
    "torch.save(ffnn_model.state_dict(), '../weights/ffnn_model_v2.pth')\n",
    "with open('../vectorisers/tfidf_vectoriser_v2.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
